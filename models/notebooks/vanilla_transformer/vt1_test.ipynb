{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vt1 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import SVG, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Add packages to python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "b_paths = [os.path.abspath(os.path.join('..', '..', '..')), os.path.abspath(os.path.join('..', '..')), os.path.abspath(os.path.join('..', '..', 'scripts'))]\n",
    "for b_path in b_paths:\n",
    "    if b_path not in sys.path:\n",
    "        sys.path.append(b_path)\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.parent.resolve()\n",
    "%cd $BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load model and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from models.scripts.generate_dataset import WordDatasetGenerator\n",
    "from models.scripts.transformer.Transformer import Transformer\n",
    "from models.scripts.transformer.utils import preprocess_dataset, seed_all, build_vocab, strokes_to_svg, load_json_hypeparameters, pad_collate_fn, tensor_to_word\n",
    "from models.scripts.utils import Levenshtein_Normalized_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"vt1\"\n",
    "SEED = 2021\n",
    "BATCH_SIZE = 256\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "hp = load_json_hypeparameters(VERSION)\n",
    "VOCAB = dict(hp['vocab'])\n",
    "VOCAB = build_vocab(a for a in VOCAB.keys() if a not in ['<pad>', '<bos>', '<eos>', '<unk>'])\n",
    "hp['vocab']=VOCAB\n",
    "N_TOKENS = len(VOCAB)\n",
    "PAD_IDX = VOCAB['<pad>']\n",
    "BOS_IDX = VOCAB['<bos>']\n",
    "EOS_IDX = VOCAB['<eos>']\n",
    "\n",
    "print(f\"Number of Tokens: {N_TOKENS}\\n\")\n",
    "print({VOCAB.itos[i]: i for i in range(N_TOKENS)}) # Token order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "d_gen = WordDatasetGenerator(vocab = VOCAB, fname=\"words_stroke_100_155805\")\n",
    "test = d_gen.generate_from_cache(mode='test')\n",
    "\n",
    "test_set = DataLoader(preprocess_dataset(test, VOCAB,  os.path.join(d_gen.fname, \"test.pt\"), total_len=d_gen.get_learning_set_length(\"test\")), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = Transformer(name=VERSION, **hp)\n",
    "model.load_best_version()\n",
    "model.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test on a single expression (0 and 1 required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "x_pred, y_pred = next(test_set_iter)\n",
    "\n",
    "x_pred = x_pred.to(model.device)\n",
    "y_pred = y_pred.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ind = random.choice(range(0, y_pred.shape[0]))\n",
    "print(\"Index:\", ind, \"\\n\")\n",
    "\n",
    "\n",
    "svg_str = strokes_to_svg(x_pred[ind], {'height':100, 'width':100}, d_gen.padding_value, BOS_IDX, EOS_IDX)\n",
    "display(SVG(data = svg_str))\n",
    "\n",
    "eos_tensor = torch.zeros(x_pred[ind].size(-1)) + EOS_IDX\n",
    "\n",
    "prediction, (cross_att, dec_att, enc_att), _ = model.predict(x_pred[ind].unsqueeze(0))\n",
    "\n",
    "gt = tensor_to_word(y_pred[ind], VOCAB)\n",
    "gt_list = [i for i in y_pred[ind].tolist() if i != 1]\n",
    "gt_length = len([i for i in y_pred[ind] if i not in [PAD_IDX, BOS_IDX, EOS_IDX]])\n",
    "\n",
    "print(\"Ground truth => \", gt_list , '\\n')\n",
    "\n",
    "# Show ground truth and prediction along with the lengths of the words/glyphs\n",
    "print(f\"Ground Truth: {''.join(gt)} (len={gt_length})\")\n",
    "print(f\"- Prediction: {''.join(prediction)} (len={len(prediction)-2})\")\n",
    "\n",
    "print(f\"Normalized Levenshtein distance is: {Levenshtein_Normalized_distance(a=''.join(gt).strip('<bos>').strip('<pad>').strip('<eos>'), b=''.join(prediction).strip('<bos>').strip('<eos>').strip('<pad>'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trace_and_export(src=x_pred[ind].unsqueeze(0), trg=y_pred[ind].unsqueeze(0), version=f\"{VERSION}_single_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate on test set (0 and 1 required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average test set cross-entropy loss (XEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_loss = model.evaluate_f(test_set)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ATTENTION #### it would need to load in memory the whole test set!!!!\n",
    "\n",
    "model.trace_and_export(src=test_set, trg=test_set, version=f\"{VERSION}_test_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Normalized Levensthein accuracy, Character Error Rate and Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "metrics = model.evaluate_multiple(test_set_iter, [\"Lev_acc\", \"CER\", \"WER\"])\n",
    "print(f\"\\nNormalized Levenshtein accuracy of test set is: {metrics['Lev_acc']}\")\n",
    "print(f\"\\nCharacter Error Rate of test set is: {metrics['CER']}\")\n",
    "print(f\"\\nWord Error Rate of test set is: {metrics['WER']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "wer = model.evaluate_WER(test_set_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\\nWord Error Rate of test set is: {wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization (0,1,2 required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Index:\", ind)\n",
    "ind_list = [i for i in prediction]\n",
    "\n",
    "model.display_encoder_self_attention(x_pred[ind], x_pred[ind], enc_att)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.display_decoder_self_attention(ind_list, ind_list, dec_att)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.display_cross_attention(x_pred[ind], ind_list, cross_att)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
