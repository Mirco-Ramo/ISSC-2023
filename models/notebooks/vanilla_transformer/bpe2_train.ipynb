{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bpe2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import normalizers\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import ByteLevel, Digits\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as torch\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Hot Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Python path\n",
    "Add the `models` directory to Python's `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "b_paths = [os.path.abspath(os.path.join('..', '..', '..')), os.path.abspath(os.path.join('..', '..')), os.path.abspath(os.path.join('..', '..', 'scripts'))]\n",
    "for b_path in b_paths:\n",
    "    if b_path not in sys.path:\n",
    "        sys.path.append(b_path)\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.parent.resolve()\n",
    "%cd $BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.scripts.transformer.Transformer import Transformer\n",
    "from models.scripts.transformer.utils import strokes_to_svg, preprocess_dataset, seed_all, pad_collate_fn\n",
    "from models.scripts.generate_dataset import WordDatasetGenerator, WordGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"bpe2\"\n",
    "SEED = 2021\n",
    "BATCH_SIZE = 256\n",
    "EXPR_MODE = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(SEED) # Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_FILE = os.path.join(\"word_sources\",\"tokenizer-news-normalized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "\n",
    "if use_cache: # Generate from cache file\n",
    "    VOCAB = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "\n",
    "    d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                                 expr_mode=EXPR_MODE,\n",
    "                                 fname=\"words_stroke_100_155805\")\n",
    "    train, valid, test = d_gen.generate_from_cache()\n",
    "\n",
    "else: # Generate from scratch and cache (if regenerated, results could change)\n",
    "    news_commentary_path = os.path.join(BASE_DIR, \"word_sources\", \"news-commentary-v14.en\")\n",
    "    words = WordGenerator().generate_from_file(news_commentary_path, words_only=False)\n",
    "\n",
    "    VOCAB = Tokenizer(BPE())\n",
    "    VOCAB.normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "    VOCAB.pre_tokenizer = pre_tokenizers.Sequence([Digits(), ByteLevel()])\n",
    "    # Train it\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=2000,\n",
    "        min_frequency=25,\n",
    "        show_progress=True,\n",
    "        special_tokens=['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "    )\n",
    "    VOCAB.train_from_iterator(words, trainer)\n",
    "    # Save the tokenizer model\n",
    "    VOCAB.save(f\"{TOKENIZER_FILE}_new\")\n",
    "    BRUSH_SPLIT=0.15\n",
    "    d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                                 expr_mode=EXPR_MODE,\n",
    "                                 words=words[:int(len(words)*(1-BRUSH_SPLIT))],\n",
    "                                 extended_dataset=False)\n",
    "    d_gen.generate()\n",
    "    d_gen.add_training_words(words[int(len(words)*(1-BRUSH_SPLIT)):])\n",
    "    train, valid, test = d_gen.generate_from_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(VOCAB.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(VOCAB.token_to_id(\"<bos>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "N_TOKENS = VOCAB.get_vocab_size() # len(VOCAB)\n",
    "print(f\"Number of Tokens: {N_TOKENS}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_set = DataLoader(preprocess_dataset(train, VOCAB,  os.path.join(d_gen.fname+\"-bpe\", \"train.pt\"), total_len=d_gen.get_learning_set_length(\"train\")), batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate_fn)\n",
    "valid_set = DataLoader(preprocess_dataset(valid, VOCAB,  os.path.join(d_gen.fname+\"-bpe\", \"valid.pt\"), total_len=d_gen.get_learning_set_length(\"valid\")), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Get random index\n",
    "x_dummy, y_dummy = next(iter(valid_set)) # Create dummy for visualization\n",
    "ind = random.choice(range(y_dummy.shape[0]))\n",
    "print(\"Index:\", ind)\n",
    "\n",
    "print()\n",
    "print(\"X Shape:\", x_dummy[ind].shape)\n",
    "# Show actual expr for first tensor\n",
    "print(\"Y Shape:\", y_dummy[ind].shape)\n",
    "print()\n",
    "print(\"Label:\", VOCAB.decode(y_dummy[ind].tolist(), False))\n",
    "\n",
    "# Get length of subplot depending on granularity (exclude bos/eos for strokes)\n",
    "svg_str = strokes_to_svg(x_dummy[ind], {'height':100, 'width':100}, d_gen.padding_value, VOCAB.token_to_id('<bos>'), VOCAB.token_to_id('<eos>'))\n",
    "display(SVG(data = svg_str))\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'X[{ind}]:', x_dummy[ind])\n",
    "print()\n",
    "\n",
    "eos_tensor = torch.zeros(x_dummy[ind].size(-1)) + d_gen.eos_idx\n",
    "\n",
    "\n",
    "for i, row in enumerate(x_dummy[ind]):\n",
    "    if torch.all(row.eq(eos_tensor)):\n",
    "        print(\"EOS is in position:\", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyper-parameters/Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model= Transformer(VERSION, VOCAB, n_tokens=N_TOKENS, encoder_name='v74-bpe', decoder_name='new')\n",
    "# model.decoder.apply(initialize_weights)\n",
    "# model.save_hyperparameters_to_json()\n",
    "model.count_parameters()\n",
    "print(f\"Encoder trainable parameters: {sum(p.numel() for p in model.encoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Decoder trainable parameters: {sum(p.numel() for p in model.decoder.parameters() if p.requires_grad):,}.\")\n",
    "print(\"\\n\\n\\n\", model)\n",
    "model.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process along with best-model caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-4\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=VOCAB.token_to_id('<pad>'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.train_loop(resume=False,\n",
    "                 train_set=train_set,\n",
    "                 valid_set=valid_set,\n",
    "                 optimizer=optimizer,\n",
    "                 scheduler=scheduler,\n",
    "                 n_epochs=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training  Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model.plot_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
