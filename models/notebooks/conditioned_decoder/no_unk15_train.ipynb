{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# no_unk15 training\n",
    "Conditionable decoder with target language, no UNK language"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as torch\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enable Hot Reload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Edit Python path\n",
    "Add the `models` directory to Python's `path`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b_paths = [os.path.abspath(os.path.join('..', '..', '..')), os.path.abspath(os.path.join('..', '..')), os.path.abspath(os.path.join('..', '..', 'scripts'))]\n",
    "for b_path in b_paths:\n",
    "    if b_path not in sys.path:\n",
    "        sys.path.append(b_path)\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.parent.resolve()\n",
    "%cd $BASE_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ignore Warnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Helpers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.scripts.transformer.MultiLang import ConditionableTransformer\n",
    "from models.scripts.transformer.utils import strokes_to_svg, seed_all, preprocess_with_lang\n",
    "from models.scripts.generate_dataset import WordDatasetGenerator, WordGenerator\n",
    "from models.scripts.defaults import Languages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VERSION = \"no_unk15\"\n",
    "SEED = 2021\n",
    "BATCH_SIZE = 256\n",
    "EXPR_MODE = 'all'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_all(SEED) # Reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabulary and dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOKENIZER_FILE = os.path.join(\"word_sources\",\"tokenizer-big_multi-normalized.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "VOCAB = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "VOCAB.add_special_tokens([f'<bos_{lang.name.lower()}>' for lang in Languages])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(VOCAB)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sorted(VOCAB.get_vocab()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(VOCAB.token_to_id(\"<pad>\"))\n",
    "print(VOCAB.token_to_id(\"<bos_en>\"))\n",
    "print(VOCAB.token_to_id(\"<bos_de>\"))\n",
    "print(VOCAB.token_to_id(\"<bos_fr>\"))\n",
    "print(VOCAB.token_to_id(\"<bos_it>\"))\n",
    "print(VOCAB.token_to_id(\"<bos_unk>\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_TOKENS = VOCAB.get_vocab_size() # len(VOCAB)\n",
    "print(f\"Number of Tokens: {N_TOKENS}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sets=[]\n",
    "valid_sets=[]\n",
    "test_sets=[]\n",
    "\n",
    "if use_cache: # Generate from cache file\n",
    "    for lang in Languages:\n",
    "        if lang == Languages.UNK:\n",
    "            continue\n",
    "        d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                                     expr_mode=EXPR_MODE,\n",
    "                                     fname=f\"words_stroke_{lang.name.lower()}_full\")\n",
    "        train, valid, test = d_gen.generate_from_cache()\n",
    "        train_sets.append(train)\n",
    "        valid_sets.append(valid)\n",
    "        test_sets.append(test)\n",
    "\n",
    "else: # Generate from scratch and cache (if regenerated, results could change)\n",
    "    for lang in Languages:\n",
    "        if lang == Languages.UNK:\n",
    "            continue\n",
    "        lower_name = lang.name.lower()\n",
    "        news_commentary_path = os.path.join(BASE_DIR, \"word_sources\", f\"news-commentary-v14.{lower_name}\")\n",
    "        words = WordGenerator().generate_from_file(news_commentary_path, words_only=False)\n",
    "\n",
    "        BRUSH_SPLIT=0.15\n",
    "        d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                                     expr_mode=EXPR_MODE,\n",
    "                                     words=words[:int(len(words)*(1-BRUSH_SPLIT))],\n",
    "                                     extended_dataset=False,\n",
    "                                     fname=f\"words_stroke_{lower_name}_full\")\n",
    "        d_gen.generate()\n",
    "        d_gen.add_training_words(words[int(len(words)*(1-BRUSH_SPLIT)):])\n",
    "        train, valid, test = d_gen.generate_from_cache()\n",
    "        train_sets.append(train)\n",
    "        valid_sets.append(valid)\n",
    "        test_sets.append(test)\n",
    "\n",
    "assert len(train_sets) == len(valid_sets) == len(test_sets) == len(Languages)-1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Dataset for PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessed_trains = []\n",
    "preprocessed_valids = []\n",
    "for i, lang in enumerate([l for l in Languages if l != Languages.UNK]):\n",
    "    lower_name = lang.name.lower()\n",
    "    d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                                     expr_mode=EXPR_MODE,\n",
    "                                     fname=f\"words_stroke_{lower_name}_full\")\n",
    "    preprocessed_trains += preprocess_with_lang(train_sets[i], VOCAB,  os.path.join(d_gen.fname+\"_lang\", \"train.pt\"), total_len=d_gen.get_learning_set_length(\"train\"), bos=VOCAB.token_to_id(f'<bos_{lower_name}>'))\n",
    "    preprocessed_valids += preprocess_with_lang(valid_sets[i], VOCAB,  os.path.join(d_gen.fname+\"_lang\", \"valid.pt\"), total_len=d_gen.get_learning_set_length(\"valid\"), bos=VOCAB.token_to_id(f'<bos_{lower_name}>'))\n",
    "\n",
    "train_set = DataLoader(preprocessed_trains, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_set = DataLoader(preprocessed_valids, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect Generated Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get random index\n",
    "x_dummy, y_dummy = next(iter(valid_set)) # Create dummy for visualization\n",
    "print(x_dummy.shape)\n",
    "ind = random.choice(range(y_dummy.shape[0]))\n",
    "print(\"Index:\", ind)\n",
    "\n",
    "print()\n",
    "print(\"X Shape:\", x_dummy[ind].shape)\n",
    "# Show actual expr for first tensor\n",
    "print(\"Y Shape:\", y_dummy[ind].shape)\n",
    "print()\n",
    "print(\"Label:\", VOCAB.decode(y_dummy[ind].tolist(), False))\n",
    "print(\"Readable Label:\", VOCAB.decode(y_dummy[ind].tolist(), False).replace(\" \",\"\").replace(\"Ä \", \" \").rstrip(\"<pad>\"))\n",
    "\n",
    "# Get length of subplot depending on granularity (exclude bos/eos for strokes)\n",
    "svg_str = strokes_to_svg(x_dummy[ind], {'height':100, 'width':100}, d_gen.padding_value, VOCAB.token_to_id('<bos>'), VOCAB.token_to_id('<eos>'))\n",
    "display(SVG(data = svg_str))\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'X[{ind}]:', x_dummy[ind])\n",
    "print()\n",
    "\n",
    "eos_tensor = torch.zeros(x_dummy[ind].size(-1)) + d_gen.eos_idx\n",
    "\n",
    "\n",
    "for i, row in enumerate(x_dummy[ind]):\n",
    "    if torch.all(row.eq(eos_tensor)):\n",
    "        print(\"EOS is in position:\", i)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Hyper-parameters/Create Transformer Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ConditionableTransformer(VERSION, VOCAB, conv_layer_name='en-en11', encoder_name='en-en11', n_tokens=N_TOKENS, hid_dim=256, dec_heads=8, dec_layers=6, dec_pf_dim=256*3)\n",
    "model.save_hyperparameters_to_json()\n",
    "model.count_parameters()\n",
    "model.requires_grad_(False)\n",
    "model.decoder.requires_grad_(True)\n",
    "model.enc_to_dec_proj.requires_grad_(True)\n",
    "print(f\"Convolution trainable parameters: {sum(p.numel() for p in model.preencoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Encoder trainable parameters: {sum(p.numel() for p in model.encoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Decoder trainable parameters: {sum(p.numel() for p in model.decoder.parameters() if p.requires_grad):,}.\")\n",
    "print(\"\\n\\n\\n\", model)\n",
    "model.to(model.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train with the 4 languages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 7e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train_loop(resume=False,\n",
    "                 train_set=train_set,\n",
    "                 valid_set=valid_set,\n",
    "                 optimizer=optimizer,\n",
    "                 scheduler=scheduler,\n",
    "                 n_epochs=4000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot Training  Logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.plot_training()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
