{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.11;0.775x0.77)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary1 = np.array([[4, 884,],\n",
    "                    [47, 14552]])\n",
    "\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=binary1, fontcolor_threshold=0.2, hide_ticks=False, show_normed=True, class_names=[\"Y\", \"N\"])\n",
    "# cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "all_axes = fig.get_axes()\n",
    "print(all_axes)\n",
    "all_axes[0].set_xlabel(\"predicted full stop\")\n",
    "all_axes[0].set_ylabel(\"true full stop\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\My Drive\\master_thesis_experiments\\glyph-recognition-experiments\n"
     ]
    }
   ],
   "source": [
    "b_paths = [os.path.abspath(os.path.join('..', '..')), os.path.abspath(os.path.join('..')), os.path.abspath(os.path.join('..', 'scripts'))]\n",
    "for b_path in b_paths:\n",
    "    if b_path not in sys.path:\n",
    "        sys.path.append(b_path)\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.resolve()\n",
    "%cd $BASE_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from models.scripts.transformer.BERT_decoder import BertTransformer\n",
    "from models.scripts.transformer.Transformer import Transformer\n",
    "from models.scripts.transformer.utils import build_vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "CHARS = list(string.ascii_letters)\n",
    "DIGITS = [i for i in string.digits]\n",
    "LOWERCASE_LETTERS = [i for i in string.ascii_lowercase]\n",
    "UPPERCASE_LETTERS = [i for i in string.ascii_uppercase]\n",
    "MISC_CHARS = ['.', ',', '!', '?', '+', '-', '/', '*', '=', '(', ')', ]\n",
    "NO_SPACE = [\"\"]\n",
    "NEW_LINE = ['\\n']\n",
    "BLANK_SPACE = [\" \"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "TOKENIZER_FILE = os.path.join(\"word_sources\", \"tokenizer-big_multi-normalized.json\")\n",
    "VOCAB = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "\n",
    "\n",
    "N_TOKENS = VOCAB.get_vocab_size()\n",
    "PAD_IDX = VOCAB.token_to_id('<pad>')\n",
    "BOS_IDX = VOCAB.token_to_id('<bos>')\n",
    "EOS_IDX = VOCAB.token_to_id('<eos>')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets...\n",
      "\n",
      "\n",
      "Initializing cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set progress: 100%|██████████| 7/7 [00:04<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test batch... (Total=7).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train set progress: 100%|██████████| 17/17 [00:09<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train batch... (Total=17).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid set progress: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed valid batch... (Total=5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached dataset file in cache\\words_cache\\words_stroke_100_29.h5\n"
     ]
    }
   ],
   "source": [
    "from models.scripts.transformer.utils import preprocess_dataset, pad_collate_fn, preprocess_with_lang\n",
    "from models.scripts.generate_dataset import WordDatasetGenerator, WordGenerator\n",
    "\n",
    "BATCH_SIZE=16\n",
    "VERSION = \"test\"\n",
    "\n",
    "words = ['Gold?',\n",
    "'It has never been easy to have. a rational conversation about the value of gold',\n",
    "'Lately with gold prices up more than over the last decade it is harder than ever',\n",
    "'Just last December (fellow economists) Martin Feldstein',\n",
    "'and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment',\n",
    "'sensibly pointing out golds risks',\n",
    "'Wouldn t you know it',\n",
    "'Since their articles appeared the price of gold has moved up still further',\n",
    "'Gold prices even hit a recordhigh recently',\n",
    "'Last December many gold/bugs were arguing that the price was inevitably headed for',\n",
    "'Now emboldened by continuing appreciation some are suggesting',\n",
    "'that gold could be headed, even higher than that',\n",
    "'One successful gold investor recently explained to me that stock prices',\n",
    "'languished for a more than a decade before the Dow Jones index crossed the mark in the early',\n",
    "'Since then the index has climbed above',\n",
    "'Now that gold has crossed the magic barrier, why cant it increase tenfold too?',\n",
    "'Admittedly getting to a much higher price for gold',\n",
    "'is not quite the leap of imagination that* it seems?',\n",
    "'After adjusting for inflation todays price is nowhere near the alltime high of = January',\n",
    "'Back then, gold hit or well over in todays dollars',\n",
    "'But January was arguably a freak peak',\n",
    "'during a period of heightened geopolitical instability',\n",
    "'At todays price is probably more than double very longterm inflation',\n",
    "'adjusted average gold prices',\n",
    "'So what could justify another huge increase in gold prices from here 3',\n",
    "'One answer of course is a complete collapse of the US dollar',\n",
    "'Mirco Ramo beautiful sexy.',\n",
    "'Houses of Parliaments are very big!',\n",
    "'Italy voted as best country in the world']\n",
    "BRUSH_SPLIT=0.15\n",
    "d_gen = WordDatasetGenerator(vocab = VOCAB,\n",
    "                             expr_mode='all',\n",
    "                             words=words,\n",
    "                             extended_dataset=False)\n",
    "d_gen.generate()\n",
    "d_gen.add_training_words(words[int(len(words)*(1-BRUSH_SPLIT)):])\n",
    "train, valid, test = d_gen.generate_from_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache\\preprocessed_datasets\\words_stroke_100_29\\train.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:01<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache\\preprocessed_datasets\\words_stroke_100_29\\valid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 20.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from models.scripts.defaults import *\n",
    "train_set = DataLoader(preprocess_dataset(train, VOCAB,  os.path.join(d_gen.fname, \"train.pt\"), total_len=d_gen.get_learning_set_length(\"train\")), batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate_fn)\n",
    "valid_set = DataLoader(preprocess_dataset(valid, VOCAB,  os.path.join(d_gen.fname, \"valid.pt\"), total_len=d_gen.get_learning_set_length(\"valid\"),), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_fn)\n",
    "#test_set = DataLoader(preprocess_dataset(test, VOCAB,  os.path.join(d_gen.fname, \"test.pt\"), total_len=d_gen.get_learning_set_length(\"test\")), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 128]) tensor([   2, 2255,   92, 1099,  495,  418, 1207, 1711,  241,  240, 1592,    3,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1]) torch.Size([100])\n",
      "tensor([[ 2.0000,  2.0000,  2.0000,  ...,  2.0000,  2.0000,  2.0000],\n",
      "        [ 0.3762,  0.3462,  0.4096,  ..., -5.0000, -5.0000, -5.0000],\n",
      "        [ 0.4806,  0.3796,  0.4825,  ..., -5.0000, -5.0000, -5.0000],\n",
      "        ...,\n",
      "        [-5.0000, -5.0000, -5.0000,  ..., -5.0000, -5.0000, -5.0000],\n",
      "        [-5.0000, -5.0000, -5.0000,  ..., -5.0000, -5.0000, -5.0000],\n",
      "        [-5.0000, -5.0000, -5.0000,  ..., -5.0000, -5.0000, -5.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\My Drive\\master_thesis_experiments\\glyph-recognition-experiments\\models\\scripts\\transformer\\utils.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yy.append(torch.tensor(y_, dtype=torch.int64))\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_set))\n",
    "print(x[0].shape, y[0], y[0].shape)\n",
    "print(x[0, :80, :50])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bert Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: transferred encoder hyperparameters will overwrite current ones for shape consistency\n",
      "WARNING: transferred conv_layer hyperparameters will overwrite current ones for shape consistency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 141,914,216 trainable parameters.\n",
      "Encoder trainable parameters: 0.\n",
      "Decoder trainable parameters: 28,366,848.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertTransformer(\"multi\", VOCAB, n_tokens=N_TOKENS, encoder_name=\"en-en11\", decoder_name=\"new\", conv_layer_name=\"en-en11\")\n",
    "#model_classic = Transformer(\"ehy\", VOCAB, n_tokens=N_TOKENS, dec_max_length=100)\n",
    "\n",
    "## TODO try with random tensors, random db samples, plug working encoder and let's see\n",
    "model.count_parameters()\n",
    "model.requires_grad_(False)\n",
    "for layer in model.decoder.bert.encoder.layer:\n",
    "    layer.crossattention.requires_grad_(True)\n",
    "\n",
    "print(f\"Encoder trainable parameters: {sum(p.numel() for p in model.encoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Decoder trainable parameters: {sum(p.numel() for p in model.decoder.parameters() if p.requires_grad):,}.\")\n",
    "model.to(model.device)\n",
    "#model_classic.to(model_classic.device)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train_loop(resume=False,\n",
    "                 train_set=train_set,\n",
    "                 valid_set=valid_set,\n",
    "                 test_set=None,\n",
    "                 optimizer=optimizer,\n",
    "                 scheduler=scheduler,\n",
    "                 criterion=criterion,\n",
    "                 n_epochs=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test_set_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[43mtest_set\u001B[49m)\n\u001B[0;32m      2\u001B[0m x_pred, y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(test_set_iter)\n\u001B[0;32m      4\u001B[0m x_pred \u001B[38;5;241m=\u001B[39m x_pred\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "x_pred, y_pred = next(test_set_iter)\n",
    "\n",
    "x_pred = x_pred.to(model.device)\n",
    "y_pred = y_pred.to(model.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 4 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.SVG object>",
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1705\" height=\"55.0\" viewBox=\"0 0 3100 100\" style=\"background-color:white\" version=\"1.1\">\n        <title>Strokes</title>\n        <desc>Stroke Sequence</desc>\n        <g fill=\"#E0E0E0\" stroke=\"none\" stroke-width=\"3.3333333333333335\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n            <rect x=\"0\" y=\"0\" width=\"3100\" height=\"100\"/>&quot;\n        </g>\n        <g fill=\"#00D000\" stroke=\"none\" fill-opacity=\"0.58\">\n            <rect x=\"0\" y=\"0\" width=\"100\" height=\"100\"/>&quot;\n        </g>\n        <g fill=\"#D00000\" stroke=\"none\" fill-opacity=\"0.58\">\n            <rect x=\"3000\" y=\"0\" width=\"100\" height=\"100\"/>&quot;\n        </g>\n        <g fill=\"none\" stroke=\"#000000\" stroke-width=\"2.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n            <rect x=\"0\" y=\"0\" width=\"3100\" height=\"100\"/>&quot;\n        </g>\n        <g fill=\"none\" stroke=\"#303030\" stroke-dasharray=\"5.555555555555555,5.555555555555555\" stroke-width=\"2.0\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n            <line x1=\"100\" y1=\"0\" x2=\"100\" y2=\"100\"/>\n\t<line x1=\"200\" y1=\"0\" x2=\"200\" y2=\"100\"/>\n\t<line x1=\"300\" y1=\"0\" x2=\"300\" y2=\"100\"/>\n\t<line x1=\"400\" y1=\"0\" x2=\"400\" y2=\"100\"/>\n\t<line x1=\"500\" y1=\"0\" x2=\"500\" y2=\"100\"/>\n\t<line x1=\"600\" y1=\"0\" x2=\"600\" y2=\"100\"/>\n\t<line x1=\"700\" y1=\"0\" x2=\"700\" y2=\"100\"/>\n\t<line x1=\"800\" y1=\"0\" x2=\"800\" y2=\"100\"/>\n\t<line x1=\"900\" y1=\"0\" x2=\"900\" y2=\"100\"/>\n\t<line x1=\"1000\" y1=\"0\" x2=\"1000\" y2=\"100\"/>\n\t<line x1=\"1100\" y1=\"0\" x2=\"1100\" y2=\"100\"/>\n\t<line x1=\"1200\" y1=\"0\" x2=\"1200\" y2=\"100\"/>\n\t<line x1=\"1300\" y1=\"0\" x2=\"1300\" y2=\"100\"/>\n\t<line x1=\"1400\" y1=\"0\" x2=\"1400\" y2=\"100\"/>\n\t<line x1=\"1500\" y1=\"0\" x2=\"1500\" y2=\"100\"/>\n\t<line x1=\"1600\" y1=\"0\" x2=\"1600\" y2=\"100\"/>\n\t<line x1=\"1700\" y1=\"0\" x2=\"1700\" y2=\"100\"/>\n\t<line x1=\"1800\" y1=\"0\" x2=\"1800\" y2=\"100\"/>\n\t<line x1=\"1900\" y1=\"0\" x2=\"1900\" y2=\"100\"/>\n\t<line x1=\"2000\" y1=\"0\" x2=\"2000\" y2=\"100\"/>\n\t<line x1=\"2100\" y1=\"0\" x2=\"2100\" y2=\"100\"/>\n\t<line x1=\"2200\" y1=\"0\" x2=\"2200\" y2=\"100\"/>\n\t<line x1=\"2300\" y1=\"0\" x2=\"2300\" y2=\"100\"/>\n\t<line x1=\"2400\" y1=\"0\" x2=\"2400\" y2=\"100\"/>\n\t<line x1=\"2500\" y1=\"0\" x2=\"2500\" y2=\"100\"/>\n\t<line x1=\"2600\" y1=\"0\" x2=\"2600\" y2=\"100\"/>\n\t<line x1=\"2700\" y1=\"0\" x2=\"2700\" y2=\"100\"/>\n\t<line x1=\"2800\" y1=\"0\" x2=\"2800\" y2=\"100\"/>\n\t<line x1=\"2900\" y1=\"0\" x2=\"2900\" y2=\"100\"/>\n\t<line x1=\"3000\" y1=\"0\" x2=\"3000\" y2=\"100\"/>\n\t\n        </g>\n        <g fill=\"red\" font-size=\"1.6666666666666667em\" font-family=\"Arial\">\n            <text x=\"100\" y=\"95.0\">0</text>\n\t<text x=\"200\" y=\"95.0\">1</text>\n\t<text x=\"300\" y=\"95.0\">2</text>\n\t<text x=\"400\" y=\"95.0\">3</text>\n\t<text x=\"500\" y=\"95.0\">4</text>\n\t<text x=\"600\" y=\"95.0\">5</text>\n\t<text x=\"700\" y=\"95.0\">6</text>\n\t<text x=\"800\" y=\"95.0\">7</text>\n\t<text x=\"900\" y=\"95.0\">8</text>\n\t<text x=\"1000\" y=\"95.0\">9</text>\n\t<text x=\"1100\" y=\"95.0\">10</text>\n\t<text x=\"1200\" y=\"95.0\">11</text>\n\t<text x=\"1300\" y=\"95.0\">12</text>\n\t<text x=\"1400\" y=\"95.0\">13</text>\n\t<text x=\"1500\" y=\"95.0\">14</text>\n\t<text x=\"1600\" y=\"95.0\">15</text>\n\t<text x=\"1700\" y=\"95.0\">16</text>\n\t<text x=\"1800\" y=\"95.0\">17</text>\n\t<text x=\"1900\" y=\"95.0\">18</text>\n\t<text x=\"2000\" y=\"95.0\">19</text>\n\t<text x=\"2100\" y=\"95.0\">20</text>\n\t<text x=\"2200\" y=\"95.0\">21</text>\n\t<text x=\"2300\" y=\"95.0\">22</text>\n\t<text x=\"2400\" y=\"95.0\">23</text>\n\t<text x=\"2500\" y=\"95.0\">24</text>\n\t<text x=\"2600\" y=\"95.0\">25</text>\n\t<text x=\"2700\" y=\"95.0\">26</text>\n\t<text x=\"2800\" y=\"95.0\">27</text>\n\t<text x=\"2900\" y=\"95.0\">28</text>\n\t<text x=\"3000\" y=\"95.0\">29</text>\n\t\n        </g>\n        <g fill=\"none\" stroke=\"black\" stroke-width=\"1.0\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n            <path d=\" M132.40740597248077,92.87037253379822 L132.08343386650085,91.71348214149475 L131.43795728683472,88.18669319152832 L130.67734241485596,79.21231985092163 L130.4629623889923,66.87852740287781 L130.56454360485077,56.30083084106445 L130.75773119926453,49.45630431175232 L130.8581918478012,44.98383700847626 L131.04518949985504,42.12659299373627 L131.18158280849457,39.995890855789185 L131.47581219673157,39.030614495277405 L131.86115026474,39.277857542037964 L132.30884671211243,40.869203209877014 L133.11423063278198,43.921712040901184 L134.79096293449402,47.446805238723755 L137.54591345787048,50.60147047042847 L139.6156221628189,52.39495635032654 L141.5436416864395,52.876585721969604 L144.09287869930267,52.16394066810608 L146.26856744289398,50.42083263397217 L148.5090285539627,47.604453563690186 L151.20415687561035,44.04245316982269 L153.1511664390564,40.03838002681732 L154.58728075027466,36.64202690124512 L155.83828687667847,34.747314453125 L156.367427110672,34.49411392211914 L156.4814805984497,35.787755250930786 L156.1217486858368,39.32742178440094 L155.64213395118713,44.531646370887756 L155.34817576408386,51.71801447868347 L155.27777671813965,59.567415714263916 L155.39786219596863,67.5158679485321 L155.94047904014587,74.29111003875732 L156.97980523109436,79.6697199344635 L157.48445391654968,84.11676287651062 L158.57903361320496,88.06576132774353 L158.61111283302307,88.61111402511597 L158.61111283302307,88.61111402511597 M242.9629623889923,60.18518805503845 L242.77777671813965,60.5555534362793 L241.8763518333435,64.67529535293579 L241.11820459365845,69.29510831832886 L240.66016674041748,73.62380623817444 L240.5147224664688,77.45397090911865 L240.7869666814804,80.23117780685425 L241.7644441127777,82.28105306625366 L243.37238371372223,83.39804410934448 L244.72222328186035,83.5185170173645 L244.72222328186035,83.5185170173645 M345.18518447875977,46.574074029922485 L345.09259164333344,46.85185253620148 L345.09259164333344,47.31481373310089 L345.09259164333344,47.31481373310089 M446.4648514986038,50.85803270339966 L446.80769443511963,51.91739201545715 L447.4871754646301,54.32329773902893 L447.5568950176239,56.439149379730225 L447.48794734477997,57.39219784736633 L447.3228871822357,57.72230625152588 L447.0561474561691,57.221561670303345 L446.03421092033386,55.349308252334595 L445.1315760612488,51.75070762634277 L444.8817640542984,47.991153597831726 L445.7966059446335,44.873276352882385 L447.8882223367691,42.255619168281555 L451.0030448436737,40.484341979026794 L455.11823892593384,39.7223562002182 L454.73318099975586,39.781591296195984 M536.7592602968216,67.22221970558167 L536.4814817905426,67.59259104728699 L534.2625856399536,70.95152735710144 L530.3443968296051,78.21317315101624 L528.2797604799271,84.38970446586609 L528.7256181240082,88.69391679763794 L530.8168262243271,91.45528078079224 L534.1742485761642,93.01449060440063 L538.20661008358,93.42592358589172 L542.4503892660141,92.97198057174683 L546.4046150445938,91.62876009941101 L547.8703707456589,90.64815044403076 L547.8703707456589,90.64815044403076 M651.3618350028992,43.99375915527344 L650.5277931690216,44.54128444194794 L648.4864354133606,46.987149119377136 L646.5210527181625,50.869953632354736 L645.7340478897095,54.40574884414673 L645.9576666355133,57.16645121574402 L647.1698194742203,58.67849588394165 L649.2055296897888,58.75771641731262 L651.4473080635071,57.61920213699341 L653.3257186412811,55.36011457443237 L654.2659521102905,52.23808288574219 L654.1073262691498,49.094197154045105 L652.7875423431396,46.703046560287476 L650.7106959819794,46.02183997631073 L648.4513491392136,46.771231293678284 L647.6176828145981,47.737908363342285 L647.6176828145981,47.737908363342285 M732.4999988079071,41.66666567325592 L732.4999988079071,42.03703701496124 L732.5350970029831,45.69528102874756 L732.8244417905807,55.48798441886902 L733.113431930542,68.46095323562622 L733.536434173584,78.62695455551147 L734.1394126415253,84.96445417404175 L734.983491897583,89.61253762245178 L735.6992423534393,92.46809482574463 L735.8796298503876,93.14814805984497 L735.8333319425583,92.96296238899231 M825.7407397031784,43.42592656612396 L825.8333325386047,43.2407408952713 L826.3446927070618,41.940245032310486 L827.9721736907959,39.67413306236267 L830.6986600160599,37.01458275318146 L833.7563693523407,34.976813197135925 L837.2286766767502,33.257514238357544 L840.7613962888718,32.499998807907104 L843.9526468515396,33.348819613456726 L846.5631812810898,35.88734269142151 L848.3900845050812,39.83198404312134 L849.1517633199692,44.628509879112244 L848.7007528543472,49.56162869930267 L847.2392320632935,54.10619378089905 L844.6720451116562,57.912009954452515 L841.7482823133469,60.81888675689697 L838.8858616352081,63.018035888671875 L836.65951192379,64.86392021179199 L835.5477333068848,66.75597429275513 L835.7886791229248,69.03269290924072 L837.1766120195389,71.81191444396973 L839.7338420152664,74.97612833976746 L843.248388171196,78.44291925430298 L847.2016662359238,82.11103677749634 L849.3518531322479,84.07407402992249 L849.3518531322479,84.07407402992249 M959.6692025661469,46.489858627319336 L955.8247089385986,45.86583375930786 L950.8138716220856,47.15850353240967 L945.8026975393295,50.131964683532715 L941.9864743947983,53.90945076942444 L939.3315613269806,58.16566348075867 L938.7643694877625,61.529093980789185 L939.8180037736893,63.936954736709595 L942.4724340438843,64.34876918792725 L946.6755837202072,62.91212439537048 L951.3646483421326,60.330843925476074 L955.5410921573639,56.9793164730072 L958.5554122924805,54.155921936035156 L960.555225610733,52.45809555053711 L961.2356305122375,52.58045196533203 L961.0661923885345,53.0633270740509 L960.5468034744263,55.98644018173218 L959.898316860199,60.48668622970581 L959.825211763382,64.23404812812805 L960.2724015712738,67.60255098342896 L960.4492366313934,68.48673820495605 L960.4492366313934,68.48673820495605 M1039.7035866975784,45.397815108299255 L1039.851513504982,47.51726984977722 L1040.177908539772,50.60906410217285 L1040.8349364995956,53.33569049835205 L1041.1404222249985,55.04530668258667 L1041.3097351789474,55.63039183616638 L1041.4196580648422,55.166810750961304 L1041.493120789528,53.93196940422058 L1041.6624248027802,50.926417112350464 L1042.0899212360382,47.45185375213623 L1042.8553193807602,44.637513160705566 L1044.1185027360916,42.57028102874756 L1045.719936490059,41.82325005531311 L1047.5031524896622,42.43331551551819 L1049.1299211978912,44.05972063541412 L1050.4211843013763,46.21290564537048 L1051.3691186904907,48.13605546951294 L1051.926463842392,49.25074279308319 L1052.2518873214722,49.16236996650696 L1052.7326345443726,48.039790987968445 L1053.328675031662,46.28048241138458 L1054.0990352630615,44.76863145828247 L1055.0487995147705,43.88745725154877 L1056.0513257980347,44.12781596183777 L1057.0315778255463,45.41665017604828 L1058.0110132694244,47.55350947380066 L1058.9889109134674,49.83527362346649 L1059.965431690216,51.71043872833252 L1060.2964103221893,52.10608243942261 L1060.2964103221893,52.10608243942261 M1148.1102854013443,43.52574050426483 L1146.4966356754303,45.285627245903015 L1145.3721016645432,48.13064932823181 L1145.5029904842377,50.93684792518616 L1146.6912418603897,52.9843807220459 L1148.6780643463135,53.79183292388916 L1151.0448932647705,53.21077108383179 L1153.3147156238556,51.260459423065186 L1154.5745849609375,48.62581491470337 L1154.627901315689,45.709991455078125 L1153.5252749919891,43.12328100204468 L1151.6177892684937,42.05624163150787 L1149.5077908039093,42.12386608123779 L1147.7416187524796,43.20653975009918 L1146.6460019350052,44.954466819763184 L1146.5502232313156,45.24180889129639 L1146.5502232313156,45.24180889129639 M1274.1029620170593,24.648985266685486 L1274.1029620170593,28.53231132030487 L1274.6469259262085,34.80471074581146 L1276.018637418747,42.25539267063141 L1277.31574177742,49.41755831241608 L1278.4542202949524,55.92589974403381 L1279.2725384235382,60.836005210876465 L1279.563182592392,63.677942752838135 L1279.563182592392,65.00421166419983 L1278.8178265094757,63.10456395149231 L1278.315132856369,59.26603078842163 L1278.9702951908112,54.66477870941162 L1280.7799816131592,51.11812353134155 L1283.3822906017303,48.84864389896393 L1286.7114663124084,48.64303767681122 L1289.833778142929,49.97698664665222 L1292.3189282417297,52.225327491760254 L1293.1426286697388,54.494524002075195 L1291.7884469032288,56.76918029785156 L1288.9069020748138,58.69953632354736 L1285.1065039634705,59.885621070861816 L1281.4707279205322,60.58927774429321 L1279.2511701583862,60.84243655204773 L1279.2511701583862,60.84243655204773 M1345.8098351955414,58.34633111953735 L1346.9194293022156,58.21802020072937 L1349.579805135727,58.190327882766724 L1352.663093805313,57.72475600242615 L1355.2680432796478,56.90559148788452 L1357.1680128574371,55.59365153312683 L1357.8579306602478,54.14769649505615 L1356.529837846756,52.88935899734497 L1353.8968741893768,52.483028173446655 L1350.1887738704681,53.19454073905945 L1346.5385884046555,55.16422986984253 L1343.57128739357,58.14139246940613 L1342.14206635952,61.396533250808716 L1342.6131755113602,64.38247561454773 L1344.62548494339,66.54872298240662 L1348.3200699090958,67.86596775054932 L1352.8959691524506,68.3455228805542 L1355.1702082157135,68.33073496818542 L1355.1702082157135,68.33073496818542 M1449.6879875659943,34.78939235210419 L1448.8241463899612,34.43547189235687 L1446.401995420456,34.088343381881714 L1443.1498140096664,35.49857139587402 L1440.5313402414322,38.227033615112305 L1438.7673646211624,41.53718650341034 L1438.2995307445526,44.839128851890564 L1438.8862490653992,47.638434171676636 L1440.3270661830902,49.92134869098663 L1442.4424558877945,50.726813077926636 L1444.902354478836,50.338369607925415 L1447.5065767765045,48.59393537044525 L1449.738097190857,46.439746022224426 L1451.129275560379,43.790486454963684 L1451.7942130565643,41.57496690750122 L1451.8720746040344,40.37884175777435 L1451.6380667686462,40.015602111816406 L1451.6914248466492,41.79319739341736 L1452.307504415512,45.12230157852173 L1453.9132475852966,49.012136459350586 L1455.8665037155151,52.09149718284607 L1458.0062329769135,55.02294301986694 L1460.0818932056427,57.59228467941284 L1461.7004692554474,59.126365184783936 L1461.7004692554474,59.126365184783936 M1543.9157575368881,36.50546073913574 L1544.2207634449005,37.650999426841736 L1544.9560075998306,40.648940205574036 L1546.185803413391,43.59816312789917 L1547.4875181913376,45.80634832382202 L1548.7887352705002,46.74226641654968 L1550.3733396530151,46.52772843837738 L1551.998484134674,45.1153963804245 L1553.3036828041077,43.00950467586517 L1554.2472064495087,40.57745635509491 L1554.9003899097443,38.49698603153229 L1554.9921989440918,37.86401152610779 L1554.7653794288635,39.72403109073639 L1554.5241832733154,42.872437834739685 L1554.8725724220276,46.16730511188507 L1555.8934330940247,48.76031279563904 L1556.0842454433441,49.1419643163681 L1556.0842454433441,49.1419643163681 M1650.2261102199554,26.52105987071991 L1648.9822000265121,28.18664312362671 L1646.5933829545975,32.74461328983307 L1644.272768497467,39.347898960113525 L1642.85309612751,45.757389068603516 L1642.2855913639069,51.96184515953064 L1642.731973528862,57.21049904823303 L1644.0378457307816,61.26881241798401 L1646.168202161789,63.982850313186646 L1649.493619799614,64.99665379524231 L1653.8330018520355,64.85268473625183 L1657.7144086360931,63.806551694869995 L1657.7144086360931,63.806551694869995 M1742.823714017868,48.36193323135376 L1742.979720234871,47.893914580345154 L1745.0061619281769,45.91180682182312 L1749.6300846338272,44.07596290111542 L1755.5527210235596,42.6194965839386 L1757.176285982132,42.27769076824188 L1757.176285982132,42.27769076824188 M1842.9629623889923,60.18518805503845 L1842.7777767181396,60.5555534362793 L1841.8763518333435,64.67529535293579 L1841.1182045936584,69.29510831832886 L1840.6601667404175,73.62380623817444 L1840.5147224664688,77.45397090911865 L1840.7869666814804,80.23117780685425 L1841.7644441127777,82.28105306625366 L1843.3723837137222,83.39804410934448 L1844.7222232818604,83.5185170173645 L1844.7222232818604,83.5185170173645 M1945.1851844787598,46.574074029922485 L1945.0925916433334,46.85185253620148 L1945.0925916433334,47.31481373310089 L1945.0925916433334,47.31481373310089 M2054.498028755188,27.925115823745728 L2053.7179946899414,28.237128257751465 L2050.4460632801056,30.667057633399963 L2047.7244228124619,34.60806608200073 L2046.1196959018707,39.328187704086304 L2045.5019742250443,44.56079602241516 L2045.5673396587372,49.776691198349 L2046.31287753582,54.73034977912903 L2047.592705488205,59.00323987007141 L2048.257777094841,60.53042411804199 L2048.257777094841,60.53042411804199 M2142.5897032022476,51.79407000541687 L2146.6782063245773,51.48205757141113 L2151.403856277466,52.137261629104614 L2155.7546973228455,52.10608243942261 L2157.41029381752,51.79407000541687 L2157.41029381752,51.79407000541687 M2241.731670498848,37.59750425815582 L2241.8622702360153,40.489017963409424 L2242.470967769623,43.40353608131409 L2243.601533770561,45.86067497730255 L2245.075872540474,47.64997363090515 L2246.704086661339,48.23316931724548 L2248.6860781908035,47.6148396730423 L2250.6402015686035,46.060359477996826 L2252.1443605422974,43.67326498031616 L2253.208553791046,41.558295488357544 L2253.8394510746,40.21500647068024 L2254.320925474167,40.155115723609924 L2254.7676622867584,41.56982898712158 L2255.415368080139,44.37113106250763 L2256.2204360961914,48.282673954963684 L2257.5198113918304,52.35663056373596 L2258.2683324813843,54.13416624069214 L2258.2683324813843,54.13416624069214 M2351.26816034317,29.173165559768677 L2350.9543240070343,30.274370312690735 L2349.976724386215,34.53831970691681 L2349.184226989746,41.33199751377106 L2348.73183965683,47.68185615539551 L2348.825803399086,53.27809453010559 L2349.0840703248978,56.942278146743774 L2349.0840703248978,56.942278146743774 M2453.198128938675,39.31357264518738 L2451.9508600234985,39.647647738456726 L2449.3663638830185,40.85959792137146 L2447.3216384649277,42.92288422584534 L2446.801871061325,45.35006284713745 L2447.375398874283,47.915253043174744 L2448.6587524414062,50.35960078239441 L2449.7911870479584,52.14806795120239 L2450.431329011917,53.59277129173279 L2449.858659505844,54.353511333465576 L2448.0918556451797,54.80319857597351 L2447.4258959293365,54.75819110870361 L2447.4258959293365,54.75819110870361 M2543.9428001642227,54.602181911468506 L2545.1154679059982,53.937846422195435 L2548.037949204445,52.2550106048584 L2551.765716075897,50.318896770477295 L2554.7169029712677,48.19868206977844 L2556.4936876296997,46.24939560890198 L2556.552469730377,44.666457176208496 L2554.786843061447,44.11102831363678 L2551.8360912799835,44.933101534843445 L2548.205780982971,47.242504358291626 L2545.1819121837616,50.63630938529968 L2543.4475272893906,54.701364040374756 L2543.6353743076324,58.46564173698425 L2545.8325654268265,61.32814288139343 L2550.1396775245667,62.667131423950195 L2553.303176164627,62.71451115608215 L2553.303176164627,62.71451115608215 M2640.483620762825,43.057721853256226 L2641.353791952133,43.63784193992615 L2644.1982358694077,45.827487111091614 L2647.9325890541077,49.35435950756073 L2651.396721601486,52.987247705459595 L2654.3819963932037,56.29895329475403 L2657.169282436371,59.11935567855835 L2659.4910323619843,61.289334297180176 L2659.516382217407,61.3104522228241 L2659.516382217407,61.3104522228241 M2757.722306251526,48.51793944835663 L2756.4909040927887,48.892053961753845 L2753.2704055309296,50.91385841369629 L2748.649501800537,54.95782494544983 L2744.214680790901,59.680479764938354 L2742.277690768242,61.93447709083557 L2742.277690768242,61.93447709083557 M2850.7800340652466,54.602181911468506 L2851.404058933258,55.22620677947998 L2853.384131193161,57.91831612586975 L2855.5793583393097,60.07375121116638 L2858.1109642982483,61.15444898605347 L2860.6342256069183,60.5987548828125 L2863.2311701774597,59.03138518333435 L2865.4584407806396,57.08445906639099 L2866.8577671051025,55.25383949279785 L2867.5793290138245,54.47086691856384 L2867.929047346115,55.33552169799805 L2867.940717935562,58.046793937683105 L2867.771077156067,62.11175322532654 L2867.600589990616,67.0361876487732 L2866.7270958423615,71.5501606464386 L2864.653742313385,75.2590537071228 L2860.6557071208954,78.21348905563354 L2854.8266172409058,80.26759624481201 L2847.822231054306,81.71095252037048 L2840.2517944574356,82.58137106895447 L2832.8856855630875,83.11375975608826 L2832.059282064438,83.15132856369019 L2832.059282064438,83.15132856369019 M2950.8580327033997,61.3104522228241 L2949.141964316368,63.02652359008789 L2949.141964316368,63.02652359008789 \"/>\n        </g>\n    </svg>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth =>  [2, 38, 60, 69, 54, 66, 4, 43, 52, 64, 66, 4, 53, 56, 52, 72, 71, 60, 57, 72, 63, 4, 70, 56, 75, 76, 12, 3] \n",
      "\n",
      "Ground Truth: bos>Mirco Ramo beautiful sexy.<eos (len=26)\n",
      "- Prediction:                                                                                                      (len=98)\n",
      "Normalized Levenshtein distance is: 0.8699551569506726\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import SVG\n",
    "from models.scripts.transformer.utils import strokes_to_svg, tensor_to_word\n",
    "import random\n",
    "from models.scripts.utils import Levenshtein_Normalized_distance\n",
    "\n",
    "ind = random.choice(range(0, y_pred.shape[0]))\n",
    "print(\"Index:\", ind, \"\\n\")\n",
    "\n",
    "\n",
    "svg_str = strokes_to_svg(x_pred[ind], {'height':100, 'width':100}, d_gen.padding_value, BOS_IDX, EOS_IDX)\n",
    "display(SVG(data = svg_str))\n",
    "\n",
    "eos_tensor = torch.zeros(x_pred[ind].size(-1)) + d_gen.eos_idx\n",
    "\n",
    "prediction, (cross_att, dec_att, enc_att), _ = model.predict(x_pred[ind].unsqueeze(0))\n",
    "\n",
    "gt = tensor_to_word(y_pred[ind], VOCAB)\n",
    "gt_list = [i for i in y_pred[ind].tolist() if i != 1]\n",
    "gt_length = len([i for i in y_pred[ind] if i not in [PAD_IDX, BOS_IDX, EOS_IDX]])\n",
    "\n",
    "print(\"Ground truth => \", gt_list , '\\n')\n",
    "\n",
    "# Show ground truth and prediction along with the lengths of the words/glyphs\n",
    "print(f\"Ground Truth: {''.join(gt).strip('<pad>')} (len={gt_length})\")\n",
    "print(f\"- Prediction: {prediction.strip('<bos>').strip('<eos>').strip('<pad>')} (len={len(prediction)-2})\")\n",
    "\n",
    "print(f\"Normalized Levenshtein distance is: {Levenshtein_Normalized_distance(a=''.join(gt).strip('<bos>').strip('<pad>').strip('<eos>'), b=prediction.strip('<bos>').strip('<eos>').strip('<pad>'))}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
