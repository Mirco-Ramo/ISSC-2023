{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# en-en11 evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import SVG, display"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Add packages to python path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b_paths = [os.path.abspath(os.path.join('..', '..', '..')), os.path.abspath(os.path.join('..', '..')), os.path.abspath(os.path.join('..', '..', 'scripts'))]\n",
    "for b_path in b_paths:\n",
    "    if b_path not in sys.path:\n",
    "        sys.path.append(b_path)\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.parent.resolve()\n",
    "%cd $BASE_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load model and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.scripts.generate_dataset import WordDatasetGenerator\n",
    "from models.scripts.transformer.PreEncoders import Conv1DTransformer\n",
    "from models.scripts.transformer.utils import preprocess_dataset, seed_all, strokes_to_svg, load_json_hypeparameters, pad_collate_fn\n",
    "from models.scripts.utils import Levenshtein_Normalized_distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VERSION = \"en-en11\"\n",
    "SEED = 2021\n",
    "BATCH_SIZE = 256\n",
    "seed_all(SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOKENIZER_FILE = os.path.join(\"word_sources\",\"tokenizer-big_en-normalized.json\")\n",
    "VOCAB = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "\n",
    "BOS_IDX = VOCAB.token_to_id('<bos>')\n",
    "EOS_IDX = VOCAB.token_to_id('<eos>')\n",
    "PAD_IDX = VOCAB.token_to_id('<pad>')\n",
    "\n",
    "N_TOKENS = VOCAB.get_vocab_size() # len(VOCAB)\n",
    "print(f\"Number of Tokens: {N_TOKENS}\\n\")\n",
    "print(sorted(VOCAB.get_vocab()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_gen = WordDatasetGenerator(vocab = VOCAB, fname=\"words_stroke_en_full\")\n",
    "test = d_gen.generate_from_cache(mode=\"test\")\n",
    "\n",
    "test_set = DataLoader(preprocess_dataset(test, VOCAB,  os.path.join(d_gen.fname+\"_en\", \"test.pt\"), total_len=d_gen.get_learning_set_length(\"test\")), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp = load_json_hypeparameters(VERSION)\n",
    "if \"vocab\" in hp:\n",
    "    hp.pop(\"vocab\")\n",
    "model = Conv1DTransformer(name=VERSION, vocab=VOCAB, **hp)\n",
    "model.count_parameters()\n",
    "print(f\"Conv trainable parameters: {sum(p.numel() for p in model.preencoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Encoder trainable parameters: {sum(p.numel() for p in model.encoder.parameters() if p.requires_grad):,}.\")\n",
    "print(f\"Decoder trainable parameters: {sum(p.numel() for p in model.decoder.parameters() if p.requires_grad):,}.\")\n",
    "model.load_best_version()\n",
    "model.to(model.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Test on a single expression (0 and 1 required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "x_pred, y_pred = next(test_set_iter)\n",
    "\n",
    "x_pred = x_pred.to(model.device)\n",
    "y_pred = y_pred.to(model.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind = random.choice(range(0, y_pred.shape[0]))\n",
    "print(\"Index:\", ind, \"\\n\")\n",
    "\n",
    "\n",
    "svg_str = strokes_to_svg(x_pred[ind], {'height':100, 'width':100}, d_gen.padding_value, BOS_IDX, EOS_IDX)\n",
    "display(SVG(data = svg_str))\n",
    "\n",
    "eos_tensor = torch.zeros(x_pred[ind].size(-1)) + d_gen.eos_idx\n",
    "\n",
    "prediction, (cross_att, dec_att, enc_att), token_ids = model.predict(x_pred[ind].unsqueeze(0))\n",
    "prediction = prediction.replace(\" \", '').replace(\"Ġ\", \" \")\n",
    "\n",
    "gt = VOCAB.decode(y_pred[ind].tolist()).replace(\" \", '').replace(\"Ġ\", \" \")\n",
    "gt_list = [i for i in y_pred[ind].tolist() if i != 1]\n",
    "gt_length = len(gt)-2\n",
    "\n",
    "print(\"Ground truth => \", gt_list , '\\n')\n",
    "\n",
    "# Show ground truth and prediction along with the lengths of the words/glyphs\n",
    "print(f\"Ground Truth: {''.join(gt)} (len={gt_length})\")\n",
    "print(f\"- Prediction: {prediction} (len={len(prediction)-2})\")\n",
    "\n",
    "print(f\"Normalized Levenshtein distance is: {Levenshtein_Normalized_distance(gt, prediction)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.trace_and_export(src=x_pred[ind].unsqueeze(0), trg=y_pred[ind].unsqueeze(0), version=f\"{VERSION}_single_test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Evaluate on test set (0 and 1 required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute average test set cross-entropy loss (XEL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss = model.evaluate_f(test_set)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### ATTENTION #### it would need to load in memory the whole test set!!!!\n",
    "\n",
    "model.trace_and_export(src=test_set, trg=test_set, version=f\"{VERSION}_test_set\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Normalized Levensthein accuracy, Character Error Rate and Word Error Rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set_iter = iter(test_set)\n",
    "metrics = model.evaluate_multiple(test_set_iter, [\"Lev_acc\", \"CER\", \"WER\"])\n",
    "print(f\"\\nNormalized Levenshtein accuracy of test set is: {metrics['Lev_acc']}\")\n",
    "print(f\"\\nCharacter Error Rate of test set is: {metrics['CER']}\")\n",
    "print(f\"\\nWord Error Rate of test set is: {metrics['WER']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Visualization (0,1,2 required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-attention visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Index:\", ind)\n",
    "model.display_encoder_self_attention(x_pred[ind], x_pred[ind], enc_att)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind_list = [VOCAB.id_to_token(i).replace(\" \", '').replace(\"Ġ\", \" \") for i in token_ids]\n",
    "model.display_decoder_self_attention(ind_list, ind_list, dec_att)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind_list = [VOCAB.id_to_token(i).replace(\" \", '').replace(\"Ġ\", \" \") for i in token_ids]\n",
    "model.display_cross_attention(x_pred[ind], ind_list, cross_att)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
